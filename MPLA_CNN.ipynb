{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aditya-Patel/Stat598-FinalProject/blob/main/MPLA_CNN_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ucb6l_M4z9M0"
      },
      "outputs": [],
      "source": [
        "# Dataset\n",
        "import yfinance as yf\n",
        "\n",
        "# Visualization\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pylab as plt\n",
        "import matplotlib.dates as mdates\n",
        "import seaborn as sns\n",
        "mpl.rcParams['figure.dpi'] = 125\n",
        "mpl.rcParams['figure.figsize'] = (10, 5)\n",
        "\n",
        "# Date Manipulation\n",
        "from datetime import datetime\n",
        "\n",
        "# PyTorch Libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# statstical testing, plotting and decompositions\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "# Set device usage to GPU if available\n",
        "RANDOM_SEED = 42\n",
        "tf.random.set_seed(RANDOM_SEED)\n",
        "device = tf.device('/device:gpu:1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Neural Network Constants\n",
        "TRAINING_EPOCHS = 500\n",
        "BATCH_SIZE = 32\n",
        "NEURON_CT = 256\n",
        "POOL_SZ = 4\n",
        "STRIDES = 1\n",
        "LEARN_RATE = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3G3XJbK5rfx"
      },
      "outputs": [],
      "source": [
        "start_date = '2020-06-01'\n",
        "end_date = '2023-12-01'\n",
        "etf_ticker = 'MLPA'\n",
        "moving_average_list = []\n",
        "etf_tickers_url = \"https://raw.githubusercontent.com/Aditya-Patel/Stat598-FinalProject/main/mlpa_full-holdings.csv\"\n",
        "crude_oil_stock_url = \"https://raw.githubusercontent.com/Aditya-Patel/Stat598-FinalProject/main/crude%20oil%20spot%20price.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ishurATN5rfx"
      },
      "source": [
        "<h1>Create joint dataset between spot price and ETF Data</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baCYAHr95rfy",
        "outputId": "dcf10ad5-e4ce-4e0c-ef59-5bf9bdbae9e8"
      },
      "outputs": [],
      "source": [
        "df_holdings = pd.read_csv(etf_tickers_url)\n",
        "df_holdings = df_holdings[(df_holdings['Name'] != 'OTHER PAYABLE & RECEIVABLES') & (df_holdings['Name'] != 'CASH')]\n",
        "df_holdings[f'Market Value ($)'] = df_holdings[f'Market Value ($)'].str.replace(',', '').astype(float)\n",
        "total_market_value = df_holdings[f'Market Value ($)'].sum()\n",
        "df_holdings['Percentage Holdings By Value'] = (df_holdings[f'Market Value ($)'] / total_market_value)\n",
        "\n",
        "df_crude_price = pd.read_csv(crude_oil_stock_url,usecols=[0, 1])\n",
        "df_crude_price['Date'] = pd.to_datetime(df_crude_price['Date'], format='%b %d, %Y')\n",
        "df_crude_price.set_index('Date', inplace=True)\n",
        "df_crude_price.rename(columns={'WTI Barrell Spot Price':'Spot Price'}, inplace=True)\n",
        "df_crude_price['Spot Price'] = df_crude_price['Spot Price'].fillna(method='ffill')\n",
        "\n",
        "all_stocks_data = yf.download(etf_ticker, start=start_date, end=end_date)\n",
        "\n",
        "# Join spot price and etf data\n",
        "all_stocks_data['Ticker'] = etf_ticker\n",
        "all_stocks_data.columns = [f'{etf_ticker}_{col}' if col not in ['Ticker', 'Date'] else col for col in all_stocks_data.columns]\n",
        "all_stocks_data = all_stocks_data.join(df_crude_price, how='left')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTs1m1kq5rfz"
      },
      "source": [
        "<h1>Load all tickers within the ETF</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72BIKTbv5rfz",
        "outputId": "3ec24716-7f92-40d8-8bb1-bccdd87a83f9"
      },
      "outputs": [],
      "source": [
        "# load all tickers part of that etf\n",
        "for ticker in df_holdings['Ticker']:\n",
        "    stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
        "    percentage_holding = df_holdings.loc[df_holdings['Ticker'] == ticker, 'Percentage Holdings By Value'].iloc[0]\n",
        "    all_stocks_data[f'{ticker}_Percent_Holding'] = percentage_holding\n",
        "    stock_data.columns = [f'{ticker}_{col}' if col != 'Ticker' else col for col in stock_data.columns]\n",
        "    all_stocks_data = all_stocks_data.join(stock_data, how='outer')\n",
        "\n",
        "all_stocks_data.fillna(0, inplace=True)\n",
        "sum_values = pd.Series(0, index=all_stocks_data.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 657
        },
        "id": "toQFsizm5rfz",
        "outputId": "0775f345-427d-4b51-a8db-b56d571c9e48"
      },
      "outputs": [],
      "source": [
        "# Get all closing values\n",
        "\n",
        "all_stocks_data[f'{etf_ticker}_Next_Close'] = all_stocks_data[f'{etf_ticker}_Close'].shift(-1)\n",
        "all_stocks_data = all_stocks_data.drop(all_stocks_data.index[-1])\n",
        "close_values = all_stocks_data[[col for col in all_stocks_data.columns if '_Close' in col or col == 'Spot Price' or col == f'{etf_ticker}_Next_Close']]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sOS7vr66HS3"
      },
      "outputs": [],
      "source": [
        "# Check correlation\n",
        "df = close_values.drop(columns=[f'{etf_ticker}_Next_Close'])\n",
        "sns.heatmap(df.corr(), annot=False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Based on the plot of the correlation matrix, we see that most of the stocks are correlated positively with each other with the exception of SMLP, NGL and USDP, which have negative correlation. We expect to see this as a good ETF consists of a variety of tickers to protect against large market swings either way."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1>Model Development<h1>\n",
        "<h2> A Dense Neural Network is developed in TensorFlow to perform future analysis based on the previous closing price trend. <h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate X and y input datasets - Since we are predicting the next day value, we use the 'Next_Close' as the target value\n",
        "y = close_values[f'{etf_ticker}_Next_Close']\n",
        "X = close_values.drop(columns=[f'{etf_ticker}_Next_Close'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data for training and validation\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=RANDOM_SEED)\n",
        "\n",
        "# Convert to tensors and prefetch\n",
        "train_df = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "test_df = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
        "\n",
        "train_df = train_df.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "test_df = test_df.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CNN - 3 Convolution Layers, 3 Dense Layers\n",
        "model_1 = keras.models.Sequential([\n",
        "    # Convolution Layer\n",
        "    keras.layers.Conv1D(input_shape=(22,1), filters=NEURON_CT/4, kernel_size=(3,), activation='relu'),\n",
        "    keras.layers.MaxPool1D(pool_size=POOL_SZ, strides=STRIDES, padding='valid'),\n",
        "    keras.layers.Conv1D(filters=NEURON_CT/2, kernel_size=(3,), activation='relu'),\n",
        "    keras.layers.MaxPool1D(pool_size=POOL_SZ, strides=STRIDES, padding='valid'),\n",
        "    keras.layers.Conv1D(filters=NEURON_CT, kernel_size=(3,), activation='relu'),\n",
        "    keras.layers.MaxPool1D(pool_size=POOL_SZ, strides=STRIDES, padding='valid'),\n",
        "    # DNN Layer\n",
        "    keras.layers.Dense(NEURON_CT, activation='relu'),\n",
        "    keras.layers.Dense(NEURON_CT/2, activation='relu'),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model_1.compile(optimizer='adam', loss='mean_squared_error', metrics=['root_mean_squared_error'])\n",
        "model_1.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CNN 1- 2 Convolution Layers, 4 Dense Layers\n",
        "model_2 = keras.models.Sequential([\n",
        "    # Convolution Layer\n",
        "    keras.layers.Conv1D(input_shape=(22,1), filters=NEURON_CT/4, kernel_size=(3,), activation='relu'),\n",
        "    keras.layers.MaxPool1D(pool_size=POOL_SZ, strides=STRIDES, padding='valid'),\n",
        "    keras.layers.Conv1D(filters=NEURON_CT, kernel_size=(3,), activation='relu'),\n",
        "    keras.layers.MaxPool1D(pool_size=POOL_SZ, strides=STRIDES, padding='valid'),\n",
        "    # DNN Layer\n",
        "    keras.layers.Dense(NEURON_CT, activation='relu'),\n",
        "    keras.layers.Dense(NEURON_CT/2, activation='relu'),\n",
        "    keras.layers.Dense(NEURON_CT/4, activation='relu'),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model_2.compile(optimizer='adam', loss='mean_squared_error', metrics=['root_mean_squared_error'])\n",
        "model_2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CNN 1- 4 Convolution Layers, 2 Dense Layers\n",
        "model_3 = keras.models.Sequential([\n",
        "    # Convolution Layer\n",
        "    keras.layers.Conv1D(input_shape=(22,1), filters=NEURON_CT/8, kernel_size=(3,), activation='relu'),\n",
        "    keras.layers.MaxPool1D(pool_size=POOL_SZ, strides=STRIDES, padding='valid'),\n",
        "    keras.layers.Conv1D(filters=NEURON_CT/4, kernel_size=(3,), activation='relu'),\n",
        "    keras.layers.MaxPool1D(pool_size=POOL_SZ, strides=STRIDES, padding='valid'),\n",
        "    keras.layers.Conv1D(filters=NEURON_CT/2, kernel_size=(3,), activation='relu'),\n",
        "    keras.layers.MaxPool1D(pool_size=POOL_SZ, strides=STRIDES, padding='valid'),\n",
        "    keras.layers.Conv1D(filters=NEURON_CT, kernel_size=(3,), activation='relu'),\n",
        "    # keras.layers.MaxPool1D(pool_size=POOL_SZ, strides=STRIDES, padding='valid'),\n",
        "    # DNN Layer\n",
        "    keras.layers.Dense(NEURON_CT, activation='relu'),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model_3.compile(optimizer='adam', loss='mean_squared_error', metrics=['root_mean_squared_error'])\n",
        "model_3.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CNN - 1 Convolution Layers, 3 Dense Layers\n",
        "model_4 = keras.models.Sequential([\n",
        "    # Convolution Layer\n",
        "    keras.layers.Conv1D(input_shape=(22,1), filters=NEURON_CT, kernel_size=(3,), activation='relu'),\n",
        "    # DNN Layer\n",
        "    keras.layers.Dense(NEURON_CT, activation='relu'),\n",
        "    keras.layers.Dense(NEURON_CT/2, activation='relu'),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model_4.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_logarithmic_error'])\n",
        "model_4.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# lr_reducer = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=LEARN_RATE, patience=5)    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fit model to training data with 20% validation split\n",
        "model_1.fit(X_train,\n",
        "            y_train,\n",
        "            epochs = TRAINING_EPOCHS,\n",
        "            batch_size = BATCH_SIZE,\n",
        "            # callbacks = [lr_reducer],\n",
        "            validation_split = 0.3)\n",
        "\n",
        "model_2.fit(X_train,\n",
        "            y_train,\n",
        "            epochs = TRAINING_EPOCHS,\n",
        "            batch_size = BATCH_SIZE,\n",
        "            # callbacks = [lr_reducer],\n",
        "            validation_split = 0.3)\n",
        "\n",
        "model_3.fit(X_train,\n",
        "            y_train,\n",
        "            epochs = TRAINING_EPOCHS,\n",
        "            batch_size = BATCH_SIZE,\n",
        "            # callbacks = [lr_reducer],\n",
        "            validation_split = 0.3)\n",
        "\n",
        "model_4.fit(X_train,\n",
        "            y_train,\n",
        "            epochs = TRAINING_EPOCHS,\n",
        "            batch_size = BATCH_SIZE,\n",
        "            # callbacks = [lr_reducer],\n",
        "            validation_split = 0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predict next close with all models\n",
        "y1_pred = [val[0] for val in [val[0] for val in model_1.predict(X_test)]]\n",
        "mse1 = mean_squared_error(y_true=y_test, y_pred=y1_pred)\n",
        "\n",
        "\n",
        "y2_pred = [val[0] for val in [val[0] for val in model_2.predict(X_test)]]\n",
        "mse2 = mean_squared_error(y_true=y_test, y_pred=y2_pred)\n",
        "\n",
        "\n",
        "y3_pred = [val[0] for val in [val[0] for val in model_3.predict(X_test)]]\n",
        "mse3 = mean_squared_error(y_true=y_test, y_pred=y3_pred)\n",
        "\n",
        "y4_pred = [val[0] for val in [val[0] for val in model_4.predict(X_test)]]\n",
        "mse4 = mean_squared_error(y_true=y_test, y_pred=y4_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_df = pd.DataFrame(y_test)\n",
        "results_df.columns = ['y_actual']\n",
        "results_df['y1_pred'] = y1_pred\n",
        "results_df['y2_pred'] = y2_pred\n",
        "results_df['y3_pred'] = y3_pred\n",
        "results_df['y4_pred'] = y4_pred\n",
        "results_df.sort_index(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f'Hyperparameters: [Training Epochs: {TRAINING_EPOCHS} || Batch Size: {BATCH_SIZE} || Neurons: {NEURON_CT}]')\n",
        "print(f'Price forecast: model_1: MSE: {mse1:.4f}')\n",
        "print(f'Price forecast: model_2: MSE: {mse2:.4f}')\n",
        "print(f'Price forecast: model_3: MSE: {mse3:.4f}')\n",
        "print(f'Price forecast: model_4: MSE: {mse4:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = [x for x in range(len(y_test))]\n",
        "plt.plot(results_df.y_actual, '.-k', label='Actual')\n",
        "plt.plot(results_df.y1_pred, '.b', label='Model 1')\n",
        "plt.plot(results_df.y2_pred, 'xg', label='Model 2')\n",
        "plt.plot(results_df.y3_pred, '.r', label='Model 3')\n",
        "plt.plot(results_df.y4_pred, '+y', label='Model 4')\n",
        "plt.title('Next Day Close Prediction')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y1_diff = results_df.y_actual - results_df.y1_pred\n",
        "y2_diff = results_df.y_actual - results_df.y2_pred\n",
        "y3_diff = results_df.y_actual - results_df.y3_pred\n",
        "y4_diff = results_df.y_actual - results_df.y4_pred\n",
        "\n",
        "plt.bar(results_df.index, y1_diff, color='blue', label='Model 1')\n",
        "plt.title('Model 1: Error')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.bar(results_df.index, y2_diff, color='green', label='Model 2')\n",
        "plt.title('Model 2: Error')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.bar(results_df.index, y3_diff, color='red', label='Model 3')\n",
        "plt.title('Model 3: Error')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.bar(results_df.index, y4_diff, color='goldenrod', label='Model 4')\n",
        "plt.title('Model 4: Error')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
